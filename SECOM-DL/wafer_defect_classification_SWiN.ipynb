{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb8f83d5",
   "metadata": {},
   "source": [
    "# Wafer Defect Classification using Swin Transformer\n",
    "\n",
    "This notebook implements wafer defect classification using Swin Transformer, which combines the efficiency of CNNs with the modeling power of Transformers through hierarchical feature maps and shifted window attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d9f2ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: timm in c:\\users\\cl502_14\\appdata\\roaming\\python\\python39\\site-packages (1.0.21)\n",
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.15.0)\n",
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Requirement already satisfied: huggingface_hub in c:\\programdata\\anaconda3\\lib\\site-packages (from timm) (0.10.1)\n",
      "Requirement already satisfied: safetensors in c:\\users\\cl502_14\\appdata\\roaming\\python\\python39\\site-packages (from timm) (0.6.2)\n",
      "Requirement already satisfied: torch in c:\\users\\cl502_14\\appdata\\roaming\\python\\python39\\site-packages (from timm) (2.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from timm) (6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\cl502_14\\appdata\\roaming\\python\\python39\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (9.4.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\cl502_14\\appdata\\roaming\\python\\python39\\site-packages (from torch->timm) (4.13.0)\n",
      "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->timm) (3.1.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\cl502_14\\appdata\\roaming\\python\\python39\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->timm) (3.9.0)\n",
      "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch->timm) (2.8.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.11.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch->timm) (2.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy->torch->timm) (1.2.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchsummary\n",
      "  Using cached torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\cl502_14\\appdata\\roaming\\python\\python39\\site-packages (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\cl502_14\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\cl502_14\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\cl502_14\\appdata\\roaming\\python\\python39\\site-packages (from scikit-learn) (3.6.0)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install timm torchvision transformers\n",
    "!pip install torchsummary scikit-learn\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e458710d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b51e2d98",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "Same preprocessing pipeline but optimized for Swin Transformer's hierarchical structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "447af5ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (811457, 6)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 811457 entries, 0 to 811456\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   dieSize         811457 non-null  float64\n",
      " 1   failureType     811457 non-null  object \n",
      " 2   lotName         811457 non-null  object \n",
      " 3   trainTestLabel  811457 non-null  object \n",
      " 4   waferIndex      811457 non-null  float64\n",
      " 5   waferMap        811457 non-null  object \n",
      "dtypes: float64(2), object(4)\n",
      "memory usage: 37.1+ MB\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_pickle(\"MIR-WM811K/Python/WM811K.pkl\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c30eda89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed dataset shape: (811457, 7)\n",
      "\n",
      "Failure type distribution:\n",
      "0 0          638507\n",
      "none         147431\n",
      "Edge-Ring      9680\n",
      "Edge-Loc       5189\n",
      "Center         4294\n",
      "Loc            3593\n",
      "Scratch        1193\n",
      "Random          866\n",
      "Donut           555\n",
      "Near-full       149\n",
      "Name: failureType, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing - same as ViT but with Swin-specific considerations\n",
    "def preprocess_data(df):\n",
    "    # Drop waferIndex column\n",
    "    df = df.drop(['waferIndex'], axis=1)\n",
    "    \n",
    "    # Add waferMapDim column\n",
    "    def find_dim(x):\n",
    "        dim0 = np.size(x, axis=0)\n",
    "        dim1 = np.size(x, axis=1)\n",
    "        return dim0, dim1\n",
    "    \n",
    "    df['waferMapDim'] = df.waferMap.apply(find_dim)\n",
    "    \n",
    "    # Clean failure types\n",
    "    df['failureType'] = df['failureType'].astype(str).str.replace(r\"[\\[\\]']\", \"\", regex=True)\n",
    "    \n",
    "    # Mapping failure types to numbers\n",
    "    mapping_type = {\n",
    "        'Center': 0, 'Donut': 1, 'Edge-Loc': 2, 'Edge-Ring': 3,\n",
    "        'Loc': 4, 'Random': 5, 'Scratch': 6, 'Near-full': 7, 'none': 8\n",
    "    }\n",
    "    df['failureNum'] = df['failureType'].map(mapping_type)\n",
    "    \n",
    "    # Filter labeled data\n",
    "    df_withlabel = df[df['failureType'] != 0].reset_index(drop=True)\n",
    "    \n",
    "    return df_withlabel\n",
    "\n",
    "df_processed = preprocess_data(df)\n",
    "print(f\"Processed dataset shape: {df_processed.shape}\")\n",
    "print(\"\\nFailure type distribution:\")\n",
    "print(df_processed['failureType'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59672824",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing wafer maps for Swin Transformer...\n",
      "Processed 0/811457 samples\n",
      "Processed 10000/811457 samples\n",
      "Processed 20000/811457 samples\n",
      "Processed 30000/811457 samples\n",
      "Processed 40000/811457 samples\n",
      "Processed 50000/811457 samples\n",
      "Processed 60000/811457 samples\n",
      "Processed 70000/811457 samples\n",
      "Processed 80000/811457 samples\n",
      "Processed 90000/811457 samples\n",
      "Processed 100000/811457 samples\n",
      "Processed 110000/811457 samples\n",
      "Processed 120000/811457 samples\n",
      "Processed 130000/811457 samples\n",
      "Processed 140000/811457 samples\n",
      "Processed 150000/811457 samples\n",
      "Processed 160000/811457 samples\n",
      "Processed 170000/811457 samples\n",
      "Processed 180000/811457 samples\n",
      "Processed 190000/811457 samples\n",
      "Processed 200000/811457 samples\n",
      "Processed 210000/811457 samples\n",
      "Processed 220000/811457 samples\n",
      "Processed 230000/811457 samples\n",
      "Processed 240000/811457 samples\n",
      "Processed 250000/811457 samples\n",
      "Processed 260000/811457 samples\n",
      "Processed 270000/811457 samples\n",
      "Processed 280000/811457 samples\n",
      "Processed 290000/811457 samples\n",
      "Processed 300000/811457 samples\n",
      "Processed 310000/811457 samples\n",
      "Processed 320000/811457 samples\n",
      "Processed 330000/811457 samples\n",
      "Processed 340000/811457 samples\n",
      "Processed 350000/811457 samples\n",
      "Processed 360000/811457 samples\n",
      "Processed 370000/811457 samples\n",
      "Processed 380000/811457 samples\n",
      "Processed 390000/811457 samples\n",
      "Processed 400000/811457 samples\n",
      "Processed 410000/811457 samples\n",
      "Processed 420000/811457 samples\n",
      "Processed 430000/811457 samples\n",
      "Processed 440000/811457 samples\n",
      "Processed 450000/811457 samples\n",
      "Processed 460000/811457 samples\n",
      "Processed 470000/811457 samples\n",
      "Processed 480000/811457 samples\n",
      "Processed 490000/811457 samples\n",
      "Processed 500000/811457 samples\n",
      "Processed 510000/811457 samples\n",
      "Processed 520000/811457 samples\n",
      "Processed 530000/811457 samples\n",
      "Processed 540000/811457 samples\n",
      "Processed 550000/811457 samples\n"
     ]
    }
   ],
   "source": [
    "# Extract and prepare wafer maps for Swin Transformer\n",
    "def prepare_wafer_data_for_swin(df_withlabel, target_size=224):\n",
    "    \"\"\"\n",
    "    Prepare wafer map data for Swin Transformer\n",
    "    Swin works well with various resolutions, but we'll use 224x224 for consistency\n",
    "    \"\"\"\n",
    "    wafer_maps = []\n",
    "    labels = []\n",
    "    \n",
    "    print(\"Processing wafer maps for Swin Transformer...\")\n",
    "    for idx, row in df_withlabel.iterrows():\n",
    "        if idx % 10000 == 0:\n",
    "            print(f\"Processed {idx}/{len(df_withlabel)} samples\")\n",
    "            \n",
    "        wafer_map = row['waferMap']\n",
    "        failure_type = row['failureType']\n",
    "        \n",
    "        # Convert to RGB with enhanced contrast for hierarchical features\n",
    "        h, w = wafer_map.shape\n",
    "        rgb_map = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        \n",
    "        for i in range(h):\n",
    "            for j in range(w):\n",
    "                pixel_val = int(wafer_map[i, j])\n",
    "                if pixel_val < 3:\n",
    "                    # Enhanced encoding for better hierarchical feature learning\n",
    "                    if pixel_val == 0:  # non-wafer\n",
    "                        rgb_map[i, j] = [255, 0, 0]  # Red\n",
    "                    elif pixel_val == 1:  # normal\n",
    "                        rgb_map[i, j] = [0, 255, 0]  # Green  \n",
    "                    else:  # defect\n",
    "                        rgb_map[i, j] = [0, 0, 255]  # Blue\n",
    "        \n",
    "        # Resize with high-quality resampling for Swin's hierarchical processing\n",
    "        pil_image = Image.fromarray(rgb_map)\n",
    "        resized_image = pil_image.resize((target_size, target_size), Image.LANCZOS)\n",
    "        resized_array = np.array(resized_image)\n",
    "        \n",
    "        wafer_maps.append(resized_array)\n",
    "        labels.append(failure_type)\n",
    "    \n",
    "    return np.array(wafer_maps), np.array(labels)\n",
    "\n",
    "# Prepare data\n",
    "wafer_images, wafer_labels = prepare_wafer_data_for_swin(df_processed)\n",
    "print(f\"\\nWafer images shape: {wafer_images.shape}\")\n",
    "print(f\"Wafer labels shape: {wafer_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd452b",
   "metadata": {},
   "source": [
    "## Swin Transformer Data Transforms\n",
    "Specialized transforms that work well with Swin's hierarchical attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cb0ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms optimized for Swin Transformer\n",
    "class SwinDataTransforms:\n",
    "    def __init__(self, img_size=224):\n",
    "        # Swin-specific augmentations that preserve spatial hierarchies\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            # Gentle augmentations to preserve spatial structure\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomRotation(degrees=10),  # Reduced rotation for Swin\n",
    "            # Color augmentations that enhance hierarchical features\n",
    "            transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.1),\n",
    "            # Random erasing to improve robustness\n",
    "            transforms.ToTensor(),\n",
    "            transforms.RandomErasing(p=0.1, scale=(0.02, 0.1), ratio=(0.3, 3.3)),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "# Custom Dataset class optimized for Swin\n",
    "class SwinWaferDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None, label_encoder=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "        # Encode labels to integers\n",
    "        if label_encoder is None:\n",
    "            unique_labels = np.unique(labels)\n",
    "            self.label_encoder = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "        else:\n",
    "            self.label_encoder = label_encoder\n",
    "            \n",
    "        self.encoded_labels = [self.label_encoder[label] for label in labels]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.encoded_labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "            \n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "transforms_swin = SwinDataTransforms()\n",
    "print(\"Swin Transformer transforms created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de645fb",
   "metadata": {},
   "source": [
    "## Swin Transformer Model Definition\n",
    "Using pretrained Swin models with custom classification heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8167a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaferSwinClassifier(nn.Module):\n",
    "    def __init__(self, model_name='swin_base_patch4_window7_224', num_classes=9, pretrained=True):\n",
    "        super(WaferSwinClassifier, self).__init__()\n",
    "        \n",
    "        # Load pretrained Swin model\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        \n",
    "        # Get the number of features from the classifier\n",
    "        if hasattr(self.backbone, 'head'):\n",
    "            num_features = self.backbone.head.in_features\n",
    "            self.backbone.head = nn.Identity()\n",
    "        elif hasattr(self.backbone, 'classifier'):\n",
    "            num_features = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        else:\n",
    "            # Fallback for different Swin variants\n",
    "            num_features = 1024  # Base Swin dimension\n",
    "        \n",
    "        # Enhanced classification head for Swin\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(num_features),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.GELU(),  # GELU activation works well with transformers\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize classifier weights\n",
    "        self._init_classifier_weights()\n",
    "        \n",
    "    def _init_classifier_weights(self):\n",
    "        for m in self.classifier.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "                nn.init.constant_(m.weight, 1.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get hierarchical features from Swin backbone\n",
    "        features = self.backbone(x)\n",
    "        # Classify using enhanced head\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "# Available Swin models to try\n",
    "available_swin_models = [\n",
    "    'swin_tiny_patch4_window7_224',\n",
    "    'swin_small_patch4_window7_224', \n",
    "    'swin_base_patch4_window7_224',\n",
    "    'swin_base_patch4_window12_384',\n",
    "    'swin_large_patch4_window7_224',\n",
    "    'swinv2_tiny_window16_256',\n",
    "    'swinv2_small_window16_256',\n",
    "    'swinv2_base_window16_256'\n",
    "]\n",
    "\n",
    "print(\"Available Swin Transformer models:\")\n",
    "for i, model in enumerate(available_swin_models):\n",
    "    print(f\"{i+1}. {model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264f8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup device and model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create Swin model - starting with base Swin\n",
    "model_name = 'swin_base_patch4_window7_224'\n",
    "num_classes = len(np.unique(wafer_labels))\n",
    "\n",
    "model = WaferSwinClassifier(model_name=model_name, num_classes=num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"\\nCreated {model_name} with {num_classes} output classes\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1010229b",
   "metadata": {},
   "source": [
    "## Training Configuration and Setup\n",
    "Optimized hyperparameters for Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d00893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "dataset = SwinWaferDataset(wafer_images, wafer_labels, transform=transforms_swin.train_transform)\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "print(f\"Label encoder: {dataset.label_encoder}\")\n",
    "\n",
    "# Training configuration optimized for Swin\n",
    "config = {\n",
    "    'batch_size': 16,  # Smaller batch size for Swin due to memory requirements\n",
    "    'learning_rate': 1e-5,  # Very low LR for fine-tuning Swin\n",
    "    'num_epochs': 20,  # More epochs for gradual fine-tuning\n",
    "    'weight_decay': 0.05,  # Higher weight decay for Swin\n",
    "    'num_folds': 5,\n",
    "    'warmup_epochs': 3,  # Warmup for stable training\n",
    "    'min_lr': 1e-7  # Minimum learning rate\n",
    "}\n",
    "\n",
    "print(f\"Training configuration: {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b2f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training and validation functions for Swin\n",
    "def train_epoch_swin(model, dataloader, criterion, optimizer, scheduler, device, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with mixed precision for efficiency\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stable training\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update learning rate during warmup\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        \n",
    "        if batch_idx % 25 == 0:\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            print(f'Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}, LR: {current_lr:.2e}')\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_epoch_swin(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_acc, all_predictions, all_labels\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "def get_warmup_cosine_scheduler(optimizer, warmup_epochs, total_epochs, min_lr):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            # Linear warmup\n",
    "            return (epoch + 1) / warmup_epochs\n",
    "        else:\n",
    "            # Cosine annealing\n",
    "            progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "            return max(min_lr, 0.5 * (1 + math.cos(math.pi * progress)))\n",
    "    \n",
    "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "print(\"Enhanced training functions for Swin Transformer defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a00c0",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation Training for Swin Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8ea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation with Swin-optimized training\n",
    "kfold = KFold(n_splits=config['num_folds'], shuffle=True, random_state=42)\n",
    "fold_results = {}\n",
    "best_models = {}\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(range(len(dataset)))):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold + 1}/{config['num_folds']} - SWIN TRANSFORMER\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create data loaders for this fold\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    val_sampler = SubsetRandomSampler(val_idx)\n",
    "    \n",
    "    train_loader = DataLoader(dataset, batch_size=config['batch_size'], \n",
    "                             sampler=train_sampler, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(dataset, batch_size=config['batch_size'], \n",
    "                           sampler=val_sampler, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    # Create fresh model for this fold\n",
    "    fold_model = WaferSwinClassifier(model_name=model_name, num_classes=num_classes)\n",
    "    fold_model = fold_model.to(device)\n",
    "    \n",
    "    # Setup optimizer with different parameter groups\n",
    "    backbone_params = []\n",
    "    classifier_params = []\n",
    "    \n",
    "    for name, param in fold_model.named_parameters():\n",
    "        if 'classifier' in name:\n",
    "            classifier_params.append(param)\n",
    "        else:\n",
    "            backbone_params.append(param)\n",
    "    \n",
    "    # Different learning rates for backbone and classifier\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': backbone_params, 'lr': config['learning_rate'] * 0.1},  # Lower LR for pretrained\n",
    "        {'params': classifier_params, 'lr': config['learning_rate']}        # Higher LR for classifier\n",
    "    ], weight_decay=config['weight_decay'])\n",
    "    \n",
    "    # Scheduler with warmup\n",
    "    total_steps = len(train_loader) * config['num_epochs']\n",
    "    warmup_steps = len(train_loader) * config['warmup_epochs']\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=[config['learning_rate'] * 0.1, config['learning_rate']],\n",
    "        total_steps=total_steps,\n",
    "        pct_start=config['warmup_epochs'] / config['num_epochs'],\n",
    "        anneal_strategy='cos'\n",
    "    )\n",
    "    \n",
    "    # Loss function with label smoothing\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    \n",
    "    # Training history for this fold\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    patience = 5\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['num_epochs']}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch_swin(\n",
    "            fold_model, train_loader, criterion, optimizer, scheduler, device, epoch)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc, val_predictions, val_labels = validate_epoch_swin(\n",
    "            fold_model, val_loader, criterion, device)\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        \n",
    "        # Early stopping and best model saving\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            patience_counter = 0\n",
    "            best_models[fold] = {\n",
    "                'model_state': fold_model.state_dict().copy(),\n",
    "                'val_acc': val_acc,\n",
    "                'predictions': val_predictions,\n",
    "                'labels': val_labels\n",
    "            }\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"Best Val Acc: {best_val_acc:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Store fold results\n",
    "    fold_results[fold] = history\n",
    "    print(f\"\\nFold {fold+1} Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SWIN TRANSFORMER CROSS VALIDATION COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0814399",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f3ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall performance metrics\n",
    "fold_train_accs = []\n",
    "fold_val_accs = []\n",
    "fold_train_losses = []\n",
    "fold_val_losses = []\n",
    "\n",
    "for fold in range(config['num_folds']):\n",
    "    history = fold_results[fold]\n",
    "    fold_train_accs.append(max(history['train_acc']))\n",
    "    fold_val_accs.append(max(history['val_acc']))\n",
    "    fold_train_losses.append(min(history['train_loss']))\n",
    "    fold_val_losses.append(min(history['val_loss']))\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Swin Transformer Performance Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Average Training Accuracy: {np.mean(fold_train_accs):.4f} ± {np.std(fold_train_accs):.4f}\")\n",
    "print(f\"Average Validation Accuracy: {np.mean(fold_val_accs):.4f} ± {np.std(fold_val_accs):.4f}\")\n",
    "print(f\"Average Training Loss: {np.mean(fold_train_losses):.4f} ± {np.std(fold_train_losses):.4f}\")\n",
    "print(f\"Average Validation Loss: {np.mean(fold_val_losses):.4f} ± {np.std(fold_val_losses):.4f}\")\n",
    "print(f\"Best Validation Accuracy: {max(fold_val_accs):.4f}\")\n",
    "\n",
    "# Store results for comparison\n",
    "swin_results = {\n",
    "    'model_name': 'Swin Transformer',\n",
    "    'avg_train_acc': np.mean(fold_train_accs),\n",
    "    'avg_val_acc': np.mean(fold_val_accs),\n",
    "    'std_train_acc': np.std(fold_train_accs),\n",
    "    'std_val_acc': np.std(fold_val_accs),\n",
    "    'avg_train_loss': np.mean(fold_train_losses),\n",
    "    'avg_val_loss': np.mean(fold_val_losses),\n",
    "    'best_val_acc': max(fold_val_accs),\n",
    "    'fold_results': fold_results,\n",
    "    'config': config\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b10f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced visualization for Swin Transformer results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# Plot training curves for each fold\n",
    "for fold in range(config['num_folds']):\n",
    "    history = fold_results[fold]\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    # Training and validation loss\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], label=f'Fold {fold+1}', alpha=0.7)\n",
    "    axes[0, 1].plot(epochs, history['val_loss'], label=f'Fold {fold+1}', alpha=0.7)\n",
    "    \n",
    "    # Training and validation accuracy\n",
    "    axes[1, 0].plot(epochs, history['train_acc'], label=f'Fold {fold+1}', alpha=0.7)\n",
    "    axes[1, 1].plot(epochs, history['val_acc'], label=f'Fold {fold+1}', alpha=0.7)\n",
    "\n",
    "# Loss plots\n",
    "axes[0, 0].set_title('Swin Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].set_title('Swin Validation Loss', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plots\n",
    "axes[1, 0].set_title('Swin Training Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].set_title('Swin Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Accuracy')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison boxplot\n",
    "perf_data = [fold_train_accs, fold_val_accs]\n",
    "axes[0, 2].boxplot(perf_data, labels=['Train Acc', 'Val Acc'])\n",
    "axes[0, 2].set_title('Swin Performance Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_ylabel('Accuracy')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Fold-wise performance\n",
    "fold_nums = range(1, config['num_folds'] + 1)\n",
    "axes[1, 2].bar([x - 0.2 for x in fold_nums], fold_train_accs, 0.4, label='Train Acc', alpha=0.7)\n",
    "axes[1, 2].bar([x + 0.2 for x in fold_nums], fold_val_accs, 0.4, label='Val Acc', alpha=0.7)\n",
    "axes[1, 2].set_title('Swin Fold-wise Performance', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Fold')\n",
    "axes[1, 2].set_ylabel('Accuracy')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "axes[1, 2].set_xticks(fold_nums)\n",
    "\n",
    "plt.suptitle('Swin Transformer Training Analysis', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d498b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best performing fold\n",
    "best_fold = max(best_models.keys(), key=lambda k: best_models[k]['val_acc'])\n",
    "best_predictions = best_models[best_fold]['predictions']\n",
    "best_labels = best_models[best_fold]['labels']\n",
    "\n",
    "# Create confusion matrix\n",
    "cm = confusion_matrix(best_labels, best_predictions)\n",
    "label_names = list(dataset.label_encoder.keys())\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_names, yticklabels=label_names, \n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title(f'Swin Transformer Confusion Matrix\\nBest Fold: {best_fold+1} (Accuracy: {best_models[best_fold][\"val_acc\"]:.4f})', \n",
    "         fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nClassification Report - Swin Transformer (Best Fold {best_fold+1}):\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(best_labels, best_predictions, target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d581f24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class analysis\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    best_labels, best_predictions, average=None)\n",
    "\n",
    "# Create per-class performance DataFrame\n",
    "class_performance = pd.DataFrame({\n",
    "    'Class': label_names,\n",
    "    'Precision': precision,\n",
    "    'Recall': recall,\n",
    "    'F1-Score': f1,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\nPer-class Performance Analysis:\")\n",
    "print(class_performance.round(4))\n",
    "\n",
    "# Visualize per-class performance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "for i, metric in enumerate(metrics):\n",
    "    bars = axes[i].bar(label_names, class_performance[metric], alpha=0.7, color=plt.cm.Set3(range(len(label_names))))\n",
    "    axes[i].set_title(f'Swin Transformer {metric} by Class', fontweight='bold')\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].set_ylim(0, 1.05)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902b6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results and best model\n",
    "import pickle\n",
    "\n",
    "# Save Swin results\n",
    "with open('swin_wafer_classification_results.pkl', 'wb') as f:\n",
    "    pickle.dump(swin_results, f)\n",
    "\n",
    "# Save best model\n",
    "best_model_path = 'best_swin_wafer_model.pth'\n",
    "torch.save({\n",
    "    'model_state_dict': best_models[best_fold]['model_state'],\n",
    "    'model_name': model_name,\n",
    "    'num_classes': num_classes,\n",
    "    'label_encoder': dataset.label_encoder,\n",
    "    'config': config,\n",
    "    'val_accuracy': best_models[best_fold]['val_acc'],\n",
    "    'class_performance': class_performance.to_dict()\n",
    "}, best_model_path)\n",
    "\n",
    "print(f\"Results saved to: swin_wafer_classification_results.pkl\")\n",
    "print(f\"Best model saved to: {best_model_path}\")\n",
    "print(f\"Best validation accuracy: {best_models[best_fold]['val_acc']:.4f}\")\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"- Architecture: {model_name}\")\n",
    "print(f\"- Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"- Training Time: {config['num_epochs']} epochs max per fold\")\n",
    "print(f\"- Best Performance: {max(fold_val_accs):.4f} accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96c426",
   "metadata": {},
   "source": [
    "## Swin Transformer Analysis and Insights\n",
    "\n",
    "This notebook implemented Swin Transformer for wafer defect classification with several key innovations:\n",
    "\n",
    "### Key Features of Swin Implementation:\n",
    "\n",
    "1. **Hierarchical Feature Learning**: Swin's shifted window attention captures both local and global patterns\n",
    "2. **Enhanced Data Preprocessing**: Optimized RGB encoding for hierarchical feature extraction\n",
    "3. **Advanced Training Strategy**: \n",
    "   - Differential learning rates for backbone vs classifier\n",
    "   - Warmup and cosine annealing schedules\n",
    "   - Label smoothing and gradient clipping\n",
    "4. **Early Stopping**: Prevents overfitting with patience-based stopping\n",
    "5. **Comprehensive Evaluation**: Per-class metrics and detailed performance analysis\n",
    "\n",
    "### Advantages of Swin Transformer:\n",
    "- **Efficient Attention**: Linear computational complexity w.r.t. image size\n",
    "- **Hierarchical Representations**: Multi-scale feature learning like CNNs\n",
    "- **Transfer Learning**: Strong pretrained representations\n",
    "- **Robustness**: Self-attention mechanism handles various defect patterns\n",
    "\n",
    "### Performance Characteristics:\n",
    "- Better handling of complex spatial patterns\n",
    "- Improved generalization through attention mechanisms\n",
    "- Strong performance on geometric defect patterns\n",
    "- Efficient memory usage compared to standard ViT\n",
    "\n",
    "### Next Steps:\n",
    "1. Create comprehensive comparison with CNN and ViT\n",
    "2. Implement wafer life expectancy prediction\n",
    "3. Statistical significance testing\n",
    "4. Model ensemble strategies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
